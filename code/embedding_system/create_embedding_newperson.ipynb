{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from os.path import join, exists\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import click\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# own script imports\n",
    "import training.ds_transformations as td\n",
    "import training.helpers as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"\"\"\n",
    "    Configuration Class in which all necessary parameters that will be used in the further process are defined.\n",
    "    \"\"\"\n",
    "    DEVICE = hp.get_device()\n",
    "    DATASET_DIR = '../../ear_dataset'\n",
    "    MODEL_DIR = '../../models/ve_g_margin_2,0.pt'\n",
    "    is_small_resize = False\n",
    "    DATABASE_FOLDER = '../../embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index   Name                \n0       konrad_von          \n1       falco_len           \n"
     ]
    }
   ],
   "source": [
    "# At this point, you select the person ( the one from whom the new images were taken) \n",
    "# from whom embeddings are to be created in the following.\n",
    "# The entrclicky is made via the index in front of the name.\n",
    "new_person = hp.choose_folder(dataset_path=Config.DATASET_DIR)\n",
    "check = exists(join(Config.DATABASE_FOLDER, new_person+'.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The embedding of falco_len might already exist. Please check the folder first!\nDo you want to continue? [Y/n]: "
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "0",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# Now it is checked whether embeddings are already stored by the person \n",
    "# and the user is informed if necessary.\n",
    "# The user then decides, by y/n confirmation, how to proceed.\n",
    "\n",
    "if check: print('The embedding of ' + new_person + ' already exists. Please check the folder first!')\n",
    "if click.confirm('Do you want to continue?', default=True): \n",
    "    pass\n",
    "else: \n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/falcolentzsch/.virtualenvs/Bachelorthesis/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torchvision.models.mobilenet.MobileNetV2' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/Users/falcolentzsch/.virtualenvs/Bachelorthesis/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n/Users/falcolentzsch/.virtualenvs/Bachelorthesis/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load the model that will be used to create the embeddings.\n",
    "model = torch.load(Config.MODEL_DIR, map_location=torch.device(Config.DEVICE))\n",
    "# Specify a set of transformations to be applied to all captured images before creating embeddings.\n",
    "transformation = td.get_transform('siamese_valid_and_test', Config.is_small_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(input_, preprocess):\n",
    "    \"\"\"\n",
    "    This method performs a series of image processing procedures. It also checks whether one of the tensor in the\n",
    "    following can be processed on the graphics card.\n",
    "    1. convert the input to gray image\n",
    "    2. perform preprocessing (in this case defined in the transformations\n",
    "    3. sizes adjustment\n",
    "    4. rearrange the tensor\n",
    "    \"\"\"\n",
    "    input_ = input_.convert(\"L\")\n",
    "    input_ = preprocess(input_)\n",
    "    input_ = input_.reshape(-1, td.get_resize(Config.is_small_resize)[0], td.get_resize(Config.is_small_resize)[1], 1)\n",
    "    input_ = input_.permute(3, 0, 1, 2)   \n",
    "    if cuda.is_available():\n",
    "        return input_.type('torch.cuda.FloatTensor')\n",
    "    else:\n",
    "        return input_.type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/falcolentzsch/.virtualenvs/Bachelorthesis/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Here, each image is now converted into an embedding. \n",
    "# First, the images are preprocessed, then processed through the network and converted into an embedding\n",
    "# Finally the Embeddings are saved in our embeddings database.\n",
    "embeddings = []\n",
    "image_list = []\n",
    "for filename in glob.glob( join(Config.DATASET_DIR, new_person, '*') ):\n",
    "    img = Image.open(filename)\n",
    "    img_processed = pipeline(img,transformation)\n",
    "    image_list.append(img_processed)\n",
    "    \n",
    "embeddings = np.array([model(Variable(i)).cpu() for i in image_list])\n",
    "    \n",
    "np.save( join(Config.DATABASE_FOLDER,new_person+'.npy'), embeddings)  "
   ]
  }
 ]
}