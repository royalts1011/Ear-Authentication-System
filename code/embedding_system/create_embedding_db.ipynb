{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from os.path import join\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# own scripts imports\n",
    "from training.helpers import get_device\n",
    "import training.ds_transformations as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"\"\"\n",
    "    Configuration Class in which all necessary parameters that will be used in the further process are defined.\n",
    "    \"\"\"\n",
    "    DEVICE = get_device()\n",
    "    DATASET_DIR = '../../ear_dataset'\n",
    "    MODEL_DIR = '../../models/ve_g_margin_2,0.pt'\n",
    "    is_small_resize = False\n",
    "    DATABASE_FOLDER = '../../embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model that will be used to create the embeddings.\n",
    "model = torch.load(Config.MODEL_DIR, map_location=torch.device(Config.DEVICE))\n",
    "# Specify a set of transformations to be applied to all captured images before creating embeddings.\n",
    "transformation = td.get_transform('siamese_valid_and_test', Config.is_small_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(input_, preprocess):\n",
    "    \"\"\"\n",
    "    This method performs a series of image processing procedures. It also checks whether one of the tensor in the\n",
    "    following can be processed on the graphics card.\n",
    "    1. convert the input to gray image\n",
    "    2. perform preprocessing (in this case defined in the transformations\n",
    "    3. sizes adjustment\n",
    "    4. rearrange the tensor\n",
    "    \"\"\"\n",
    "    input_ = input_.convert(\"L\")\n",
    "    input_ = preprocess(input_)\n",
    "    input_ = input_.reshape(-1, td.get_resize(Config.is_small_resize)[0], td.get_resize(Config.is_small_resize)[1], 1)\n",
    "    input_ = input_.permute(3, 0, 1, 2)   \n",
    "    if cuda.is_available():\n",
    "        return input_.type('torch.cuda.FloatTensor')\n",
    "    else:\n",
    "        return input_.type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/falcolentzsch/.virtualenvs/Bachelorthesis/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Here, each image is now converted into an embedding. \n",
    "# First, the images are preprocessed, then processed through the network and converted into an embedding\n",
    "# Finally the Embeddings are saved in our embeddings database.\n",
    "for label in os.listdir(Config.DATASET_DIR):\n",
    "    embeddings = []\n",
    "    image_list = []\n",
    "    for filename in glob.glob( join(Config.DATASET_DIR, label, '*') ):\n",
    "        img = Image.open(filename)\n",
    "        img_processed = pipeline(img,transformation)\n",
    "        image_list.append(img_processed)\n",
    "        \n",
    "    embeddings = np.array([model(Variable(i)).cpu() for i in image_list])\n",
    "        \n",
    "    np.save( join(Config.DATABASE_FOLDER,label+'.npy'), embeddings)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}