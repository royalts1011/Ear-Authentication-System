{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "\n",
    "# own script imports\n",
    "from training.helpers import get_device\n",
    "from training.helpers import cuda_conv\n",
    "import training.ds_transformations as td\n",
    "import training.metrics as M\n",
    "import ear_detector.acquire_ear_dataset as a\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"\"\"\n",
    "    Configuration Class in which all necessary parameters that will be used in the further process are defined.\n",
    "    \"\"\"\n",
    "    DEVICE = get_device()\n",
    "    DATASET_DIR = '../../ear_dataset/'\n",
    "    AUTH_DATASET_DIR = '../../auth_data_example/unknown-auth/'\n",
    "    MODEL_DIR = '../../models/ve_g_margin_2,0.pt'\n",
    "    is_small_resize = False\n",
    "    DATABASE_FOLDER = '../../embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model that will be used to during the authentication process.\n",
    "model = torch.load(Config.MODEL_DIR, map_location=torch.device(Config.DEVICE))\n",
    "# Specify a set of transformations to be applied to all images during the authentication process.\n",
    "transformation = td.get_transform('siamese_valid_and_test', Config.is_small_resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(input_, preprocess):\n",
    "    \"\"\"\n",
    "    This method performs a series of image processing procedures. It also checks whether one of the tensor in the\n",
    "    following can be processed on the graphics card.\n",
    "    1. convert the input to gray image\n",
    "    2. perform preprocessing (in this case defined in the transformations\n",
    "    3. sizes adjustment\n",
    "    4. rearrange the tensor\n",
    "    \"\"\"\n",
    "    input_ = input_.convert(\"L\")\n",
    "    input_ = preprocess(input_)\n",
    "    input_ = input_.reshape(-1, td.get_resize(Config.is_small_resize)[0], td.get_resize(Config.is_small_resize)[1], 1)\n",
    "    input_ = input_.permute(3, 0, 1, 2)   \n",
    "    if cuda.is_available():\n",
    "        return input_.type('torch.cuda.FloatTensor')\n",
    "    else:\n",
    "        return input_.type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At this point, an image must be stored in the folder(AUTH_DATASET_DIR) for this notebook for testing, since no images are recorded here. This image is then also pre-processed and processed by the network. As a result you receive 1 Embedding. This embedding is now compared with the entire embeddings database. For each person, the smallest distance is stored.\n",
    "\n",
    "At this point, a better approach would be not to store the least distance of each person, but to calculate the average of the distances to all embeddings of a person.\n",
    "This would make the system more robust against outliers.\n",
    "\"\"\"\n",
    "\n",
    "result_value = []\n",
    "result_label = []\n",
    "\n",
    "img = Image.open(Config.AUTH_DATASET_DIR + 'unknown003.png')\n",
    "new_embedding = model(Variable(pipeline(img,transformation))).cpu()\n",
    "\n",
    "for label in os.listdir(Config.DATABASE_FOLDER):\n",
    "    loaded_embedding = np.load(Config.DATABASE_FOLDER+label, allow_pickle=True)\n",
    "    tmp = []    \n",
    "    for embedding in loaded_embedding:\n",
    "        dis = F.pairwise_distance(embedding,new_embedding)\n",
    "        tmp.append(dis.item())\n",
    "    result_value.append((min(tmp)))\n",
    "    result_label.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Listing of the 10 closest distances and the associated people.\n",
    "result_value, result_label = zip(*sorted(zip(result_value, result_label)))\n",
    "result_value = result_value[:10]\n",
    "result_label = result_label[:10]\n",
    "\n",
    "for idx, val in enumerate(result_label):\n",
    "    print(str(idx+1) + ' : ' + ' ' + val + ' : ' + ' ' + str(result_value[idx]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}